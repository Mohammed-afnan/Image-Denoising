{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"C-VAE ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPUgko01D0HAQlNNq32Xo4W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":320},"id":"68KGo0RcQ3Qg","executionInfo":{"status":"ok","timestamp":1650485426176,"user_tz":420,"elapsed":4383,"user":{"displayName":"Mohammed Afnan","userId":"03736486741766849937"}},"outputId":"0d860927-6923-4235-aba4-2ed203150c4a"},"source":["import keras\n","from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n","#from keras.layers import BatchNormalization\n","from keras.models import Model\n","from keras.datasets import mnist\n","from keras import backend as K\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Load MNIST\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","#Normalize and reshape ============\n","\n","#Norm.\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train = x_train / 255\n","x_test = x_test / 255\n","\n","# Reshape \n","img_width  = x_train.shape[1]\n","img_height = x_train.shape[2]\n","num_channels = 1 #MNIST --> grey scale so 1 channel\n","x_train = x_train.reshape(x_train.shape[0], img_height, img_width, num_channels)\n","x_test = x_test.reshape(x_test.shape[0], img_height, img_width, num_channels)\n","input_shape = (img_height, img_width, num_channels)\n","# ========================\n","#View a few images\n","plt.figure(1)\n","plt.subplot(221)\n","plt.imshow(x_train[42][:,:,0])\n","\n","plt.subplot(222)\n","plt.imshow(x_train[420][:,:,0])\n","\n","plt.subplot(223)\n","plt.imshow(x_train[4200][:,:,0])\n","\n","plt.subplot(224)\n","plt.imshow(x_train[42000][:,:,0])\n","plt.show()"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 4 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"VB4QjYcSQ8Zq","executionInfo":{"status":"ok","timestamp":1650485426176,"user_tz":420,"elapsed":35,"user":{"displayName":"Mohammed Afnan","userId":"03736486741766849937"}}},"source":["# BUILD THE MODEL\n","\n","# # ================= #############\n","# # Encoder\n","#Let us define 4 conv2D, flatten and then dense\n","# # ================= ############\n","\n","latent_dim = 2 # Number of latent dim parameters\n","\n","input_img = Input(shape=input_shape, name='encoder_input')\n","x = Conv2D(32, 3, padding='same', activation='relu')(input_img)\n","x = Conv2D(64, 3, padding='same', activation='relu',strides=(2, 2))(x)\n","x = Conv2D(64, 3, padding='same', activation='relu')(x)\n","x = Conv2D(64, 3, padding='same', activation='relu')(x)\n","\n","conv_shape = K.int_shape(x) #Shape of conv to be provided to decoder\n","#Flatten\n","x = Flatten()(x)\n","x = Dense(32, activation='relu')(x)\n","\n","# Two outputs, for latent mean and log variance (std. dev.)\n","#Use these to sample random variables in latent space to which inputs are mapped. \n","z_mu = Dense(latent_dim, name='latent_mu')(x)   #Mean values of encoded input\n","z_sigma = Dense(latent_dim, name='latent_sigma')(x)  #Std dev. (variance) of encoded input"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YzujdzzQRG_D","executionInfo":{"status":"ok","timestamp":1650485426177,"user_tz":420,"elapsed":34,"user":{"displayName":"Mohammed Afnan","userId":"03736486741766849937"}},"outputId":"11dde870-1dbb-4b98-a055-6a296b61c7c1"},"source":["#REPARAMETERIZATION TRICK\n","# Define sampling function to sample from the distribution\n","# Reparameterize sample based on the process defined by Gunderson and Huang\n","# into the shape of: mu + sigma squared x eps\n","#This is to allow gradient descent to allow for gradient estimation accurately. \n","def sample_z(args):\n","  z_mu, z_sigma = args\n","  eps = K.random_normal(shape=(K.shape(z_mu)[0], K.int_shape(z_mu)[1]))\n","  return z_mu + K.exp(z_sigma / 2) * eps\n","\n","# sample vector from the latent distribution\n","# z is the labda custom layer we are adding for gradient descent calculations\n","  # using mu and variance (sigma)\n","z = Lambda(sample_z, output_shape=(latent_dim, ), name='z')([z_mu, z_sigma])\n","\n","#Z (lambda layer) will be the last layer in the encoder.\n","# Define and summarize encoder model.\n","encoder = Model(input_img, [z_mu, z_sigma, z], name='encoder')\n","print(encoder.summary())"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"encoder\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," encoder_input (InputLayer)     [(None, 28, 28, 1)]  0           []                               \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 28, 28, 32)   320         ['encoder_input[0][0]']          \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 14, 14, 64)   18496       ['conv2d[0][0]']                 \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 14, 14, 64)   36928       ['conv2d_1[0][0]']               \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 14, 14, 64)   36928       ['conv2d_2[0][0]']               \n","                                                                                                  \n"," flatten (Flatten)              (None, 12544)        0           ['conv2d_3[0][0]']               \n","                                                                                                  \n"," dense (Dense)                  (None, 32)           401440      ['flatten[0][0]']                \n","                                                                                                  \n"," latent_mu (Dense)              (None, 2)            66          ['dense[0][0]']                  \n","                                                                                                  \n"," latent_sigma (Dense)           (None, 2)            66          ['dense[0][0]']                  \n","                                                                                                  \n"," z (Lambda)                     (None, 2)            0           ['latent_mu[0][0]',              \n","                                                                  'latent_sigma[0][0]']           \n","                                                                                                  \n","==================================================================================================\n","Total params: 494,244\n","Trainable params: 494,244\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OaBRdKJBRK8M","executionInfo":{"status":"ok","timestamp":1650485427164,"user_tz":420,"elapsed":1010,"user":{"displayName":"Mohammed Afnan","userId":"03736486741766849937"}},"outputId":"e243b3ef-b635-446a-9805-8ccf24a7a19a"},"source":["# Decoder\n","#\n","# ================= #################\n","\n","# decoder takes the latent vector as input\n","decoder_input = Input(shape=(latent_dim, ), name='decoder_input')\n","\n","# Need to start with a shape that can be remapped to original image shape as\n","#we want our final utput to be same shape original input.\n","#So, add dense layer with dimensions that can be reshaped to desired output shape\n","x = Dense(conv_shape[1]*conv_shape[2]*conv_shape[3], activation='relu')(decoder_input)\n","# reshape to the shape of last conv. layer in the encoder, so we can \n","x = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n","# upscale (conv2D transpose) back to original shape\n","# use Conv2DTranspose to reverse the conv layers defined in the encoder\n","x = Conv2DTranspose(32, 3, padding='same', activation='relu',strides=(2, 2))(x)\n","#Can add more conv2DTranspose layers, if desired. \n","#Using sigmoid activation\n","x = Conv2DTranspose(num_channels, 3, padding='same', activation='sigmoid', name='decoder_output')(x)\n","\n","# Define and summarize decoder model\n","decoder = Model(decoder_input, x, name='decoder')\n","decoder.summary()\n","\n","# apply the decoder to the latent sample \n","z_decoded = decoder(z)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"decoder\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," decoder_input (InputLayer)  [(None, 2)]               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 12544)             37632     \n","                                                                 \n"," reshape (Reshape)           (None, 14, 14, 64)        0         \n","                                                                 \n"," conv2d_transpose (Conv2DTra  (None, 28, 28, 32)       18464     \n"," nspose)                                                         \n","                                                                 \n"," decoder_output (Conv2DTrans  (None, 28, 28, 1)        289       \n"," pose)                                                           \n","                                                                 \n","=================================================================\n","Total params: 56,385\n","Trainable params: 56,385\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPvTzDzqRQ5o","executionInfo":{"status":"ok","timestamp":1650485427164,"user_tz":420,"elapsed":20,"user":{"displayName":"Mohammed Afnan","userId":"03736486741766849937"}},"outputId":"5621f890-578d-4fe4-fd67-a49f249928af"},"source":["# =========================\n","#Define custom loss\n","#VAE is trained using two loss functions reconstruction loss and KL divergence\n","#Let us add a class to define a custom layer with loss\n","class CustomLayer(keras.layers.Layer):\n","\n","    def vae_loss(self, x, z_decoded):\n","        x = K.flatten(x)\n","        z_decoded = K.flatten(z_decoded)\n","        \n","        # Reconstruction loss (as we used sigmoid activation we can use binarycrossentropy)\n","        recon_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n","        \n","        # KL divergence\n","        kl_loss = -5e-4 * K.mean(1 + z_sigma - K.square(z_mu) - K.exp(z_sigma), axis=-1)\n","        return K.mean(recon_loss + kl_loss)\n","\n","    # add custom loss to the class\n","    def call(self, inputs):\n","        x = inputs[0]\n","        z_decoded = inputs[1]\n","        loss = self.vae_loss(x, z_decoded)\n","        self.add_loss(loss, inputs=inputs)\n","        return x\n","\n","# apply the custom loss to the input images and the decoded latent distribution sample\n","y = CustomLayer()([input_img, z_decoded])\n","# y is basically the original image after encoding input img to mu, sigma, z\n","# and decoding sampled z values.\n","#This will be used as output for vae\n","\n","# =================\n","# VAE \n","# =================\n","vae = Model(input_img, y, name='vae')\n","\n","# Compile VAE\n","vae.compile(optimizer='adam')\n","vae.summary()"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"vae\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," encoder_input (InputLayer)     [(None, 28, 28, 1)]  0           []                               \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 28, 28, 32)   320         ['encoder_input[0][0]']          \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 14, 14, 64)   18496       ['conv2d[0][0]']                 \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 14, 14, 64)   36928       ['conv2d_1[0][0]']               \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 14, 14, 64)   36928       ['conv2d_2[0][0]']               \n","                                                                                                  \n"," flatten (Flatten)              (None, 12544)        0           ['conv2d_3[0][0]']               \n","                                                                                                  \n"," dense (Dense)                  (None, 32)           401440      ['flatten[0][0]']                \n","                                                                                                  \n"," latent_mu (Dense)              (None, 2)            66          ['dense[0][0]']                  \n","                                                                                                  \n"," latent_sigma (Dense)           (None, 2)            66          ['dense[0][0]']                  \n","                                                                                                  \n"," z (Lambda)                     (None, 2)            0           ['latent_mu[0][0]',              \n","                                                                  'latent_sigma[0][0]']           \n","                                                                                                  \n"," decoder (Functional)           (None, 28, 28, 1)    56385       ['z[0][0]']                      \n","                                                                                                  \n"," custom_layer (CustomLayer)     (None, 28, 28, 1)    0           ['encoder_input[0][0]',          \n","                                                                  'decoder[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 550,629\n","Trainable params: 550,629\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":796},"id":"_6QuHOQSRUqu","executionInfo":{"status":"error","timestamp":1650485428004,"user_tz":420,"elapsed":849,"user":{"displayName":"Mohammed Afnan","userId":"03736486741766849937"}},"outputId":"d119a1bb-5bd1-4fc8-9e96-2ef15146004d"},"source":["vae.fit(x_train, None, epochs = 10, batch_size = 32)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-6eac3f521053>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 919, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 240, in __call__\n        total_loss_metric_value, sample_weight=batch_dim)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/metrics_utils.py\", line 70, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/metrics.py\", line 178, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/metrics.py\", line 471, in update_state  **\n        update_total_op = self.total.assign_add(value_sum)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/keras_tensor.py\", line 255, in __array__\n        f'You are passing {self}, an intermediate Keras symbolic input/output, '\n\n    TypeError: You are passing KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='tf.math.reduce_sum/Sum:0', description=\"created by layer 'tf.math.reduce_sum'\"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.\n"]}]},{"cell_type":"code","metadata":{"id":"enGzqLpIRZ-x","executionInfo":{"status":"aborted","timestamp":1650485427997,"user_tz":420,"elapsed":18,"user":{"displayName":"Mohammed Afnan","userId":"03736486741766849937"}}},"source":["# Visualize results\n","# =================\n","#Visualize inputs mapped to the Latent space\n","#Remember that we have encoded inputs to latent space dimension = 2. \n","#Extract z_mu --> first parameter in the result of encoder prediction representing mean\n","\n","mu, _, _ = encoder.predict(x_test)\n","#Plot dim1 and dim2 for mu\n","plt.figure(figsize=(10, 10))\n","plt.scatter(mu[:, 0], mu[:, 1], c=y_test, cmap='brg')\n","plt.xlabel('dim 1')\n","plt.ylabel('dim 2')\n","plt.colorbar()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3j_-V4dzRang","executionInfo":{"status":"aborted","timestamp":1650485427999,"user_tz":420,"elapsed":20,"user":{"displayName":"Mohammed Afnan","userId":"03736486741766849937"}}},"source":["# Visualize images\n","#Single decoded image with random input latent vector (of size 1x2)\n","#Latent space range is about -5 to 5 so pick random values within this range\n","#Try starting with -1, 1 and slowly go up to -1.5,1.5 and see how it morphs from \n","#one image to the other.\n","sample_vector = np.array([[0.01,-0.01]])\n","decoded_example = decoder.predict(sample_vector)\n","decoded_example_reshaped = decoded_example.reshape(img_width, img_height)\n","plt.imshow(decoded_example_reshaped)\n","\n","#Let us automate this process by generating multiple images and plotting\n","#Use decoder to generate images by tweaking latent variables from the latent space\n","#Create a grid of defined size with zeros. \n","#Take sample from some defined linear space. In this example range [-4, 4]\n","#Feed it to the decoder and update zeros in the figure with output."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PAqTrSQXRfGD","executionInfo":{"status":"aborted","timestamp":1650485428001,"user_tz":420,"elapsed":21,"user":{"displayName":"Mohammed Afnan","userId":"03736486741766849937"}}},"source":["n = 20  # generate 15x15 digits\n","figure = np.zeros((img_width * n, img_height * n, num_channels))\n","\n","#Create a Grid of latent variables, to be provided as inputs to decoder.predict\n","#Creating vectors within range -5 to 5 as that seems to be the range in latent space\n","grid_x = np.linspace(-5, 5, n)\n","grid_y = np.linspace(-5, 5, n)[::-1]\n","\n","# decoder for each square in the grid\n","for i, yi in enumerate(grid_y):\n","    for j, xi in enumerate(grid_x):\n","        z_sample = np.array([[xi, yi]])\n","        x_decoded = decoder.predict(z_sample)\n","        digit = x_decoded[0].reshape(img_width, img_height, num_channels)\n","        figure[i * img_width: (i + 1) * img_width,\n","               j * img_height: (j + 1) * img_height] = digit\n","\n","plt.figure(figsize=(10, 10))\n","#Reshape for visualization\n","fig_shape = np.shape(figure)\n","figure = figure.reshape((fig_shape[0], fig_shape[1]))\n","\n","plt.imshow(figure, cmap='gnuplot2')\n","plt.show()  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y3Vc4ovQYsAa","executionInfo":{"status":"aborted","timestamp":1650485428002,"user_tz":420,"elapsed":21,"user":{"displayName":"Mohammed Afnan","userId":"03736486741766849937"}}},"source":[""],"execution_count":null,"outputs":[]}]}